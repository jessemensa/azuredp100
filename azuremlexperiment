EXPERIMENT WITH AZURE MACHINE LEARNING 

- FIND BEST CLASSIFICATION MODEL WITH AUTOMATED MACHINE LEARNING 


Preprocess data and configure featurization

Before you can run an automated machine learning (AutoML) experiment, you need to prepare your data. When you want to train a classification model, you'll only need to provide the training data.

After you've collected the data, you need to create a data asset in Azure Machine Learning. In order for AutoML to understand how to read the data, you need to create a MLTable data asset that includes the schema of the data.

You can create a MLTable data asset when your data is stored in a folder together with a MLTable file. When you have created the data asset, you can specify it as input with the following code:

from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import Input

my_training_data_input = Input(type=AssetTypes.MLTABLE, path="azureml:input-data-automl:1")


Once you've created the data asset, you can configure the AutoML experiment. Before AutoML trains a classification model, preprocessing transformations can be applied to your data.

Understand scaling and normalization
AutoML applies scaling and normalization to numeric data automatically, helping prevent any large-scale features from dominating training. During an AutoML experiment, multiple scaling or normalization techniques will be applied.

Configure optional featurization
You can choose to have AutoML apply preprocessing transformations, such as:

Missing value imputation to eliminate nulls in the training dataset.
Categorical encoding to convert categorical features to numeric indicators.
Dropping high-cardinality features, such as record IDs.
Feature engineering (for example, deriving individual date parts from DateTime features)
By default, AutoML will perform featurization on your data. You can disable it if you don't want the data to be transformed.

If you do want to make use of the integrated featurization function, you can customize it. For example, you can specify which imputation method should be used for a specific feature.

After an AutoML experiment is completed, you'll be able to review which scaling and normalization methods have been applied. You'll also get notified if AutoML has detected any issues with the data, like whether there are missing values or class imbalance.





RUN AN AUTOMATED MACHINE LEARNING EXPERIMENT 

To run an automated machine learning (AutoML) experiment, 
you can configure and submit the job with the Python SDK.

The algorithms AutoML uses will depend on the task you specify. 
When you want to train a classification model, 
AutoML will choose from a list of classification algorithms:

Logistic Regression
Light Gradient Boosting Machine (GBM)
Decision Tree
Random Forest
Naive Bayes
Linear Support Vector Machine (SVM)
XGBoost



Restrict algorithm selection
By default, AutoML will randomly select from the full range of algorithms for the specified task. You can choose to block individual algorithms from being selected; which can be useful if you know that your data isn't suited to a particular type of algorithm. You also may want to block certain algorithms if you have to comply with a policy that restricts the type of machine learning algorithms you can use in your organization.

Configure an AutoML experiment
When you use the Python SDK (v2) to configure an AutoML experiment or job, you configure the experiment using the automl class. For classification, you'll use the automl.classification function as shown in the following example:

from azure.ai.ml import automl

# configure the classification job
classification_job = automl.classification(
    compute="aml-cluster",
    experiment_name="auto-ml-class-dev",
    training_data=my_training_data_input,
    target_column_name="Diabetic",
    primary_metric="accuracy",
    n_cross_validations=5,
    enable_model_explainability=True
)


Specify the primary metric
One of the most important settings you must specify is the primary_metric. The primary metric is the target performance metric for which the optimal model will be determined. Azure Machine Learning supports a set of named metrics for each type of task.

To retrieve the list of metrics available when you want to train a classification model, you can use the ClassificationPrimaryMetrics function as shown here:
from azure.ai.ml.automl import ClassificationPrimaryMetrics
 
list(ClassificationPrimaryMetrics)


Set the limits
Training machine learning models will cost compute. To minimize costs and time spent on training, you can set limits to an AutoML experiment or job by using set_limits().

There are several options to set limits to an AutoML experiment:

timeout_minutes: Number of minutes after which the complete AutoML experiment is terminated.
trial_timeout_minutes: Maximum number of minutes one trial can take.
max_trials: Maximum number of trials, or models that will be trained.
enable_early_termination: Whether to end the experiment if the score isn't improving in the short term.

classification_job.set_limits(
    timeout_minutes=60, 
    trial_timeout_minutes=20, 
    max_trials=5,
    enable_early_termination=True,
)


To save time, you can also run multiple trials in parallel. When you use a compute cluster, you can have as many parallel trials as you have nodes. The maximum number of parallel trials is therefore related to the maximum number of nodes your compute cluster has. If you want to set the maximum number of parallel trials to be less than the maximum number of nodes, you can use max_concurrent_trials.


Set the training properties
AutoML will try various combinations of featurization and algorithms to train a machine learning model. If you already know that certain algorithms aren't well-suited for your data, you can exclude (or include) a subset of the available algorithms.

You can also choose whether you want to allow AutoML to use ensemble models.


Submit an AutoML experiment
You can submit an AutoML job with the following code:

# submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    classification_job
)


You can monitor AutoML job runs in the Azure Machine Learning studio. To get a direct link to the AutoML job by running the following code:
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)


EVALUATE AND COMPARE MODELS 
When an automated machine learning (AutoML) experiment has completed, you'll want to review the models that have been trained and decide which one performed best.

In the Azure Machine Learning studio, you can select an AutoML experiment to explore its details.

On the Overview page of the AutoML experiment run, you can review the input data asset and the summary of the best model. To explore all models that have been trained, you can select the Models tab:


Explore preprocessing steps
When you've enabled featurization for your AutoML experiment, data guardrails will automatically be applied too. 
The three data guardrails that are supported for classification models are:

Class balancing detection.
Missing feature values imputation.
High cardinality feature detection.
Each of these data guardrails will show one of three possible states:

Passed: No problems were detected and no action is required.
Done: Changes were applied to your data. You should review the changes AutoML has made to your data.
Alerted: An issue was detected but couldn't be fixed. You should review the data to fix the issue.
Next to data guardrails, AutoML can apply scaling and normalization techniques to each model that is trained. You can review the technique applied in the list of models under Algorithm name.

For example, the algorithm name of a model listed may be MaxAbsScaler, LightGBM. MaxAbsScaler refers to a scaling technique where each feature is scaled by its maximum absolute value. LightGBM refers to the classification algorithm used to train the model.



Retrieve the best run and its model
When you're reviewing the models in AutoML, you can easily identify the best run based on the primary metric you specified. 
In the Azure Machine Learning studio, the models are automatically sorted to show the best performing model at the top.

In the Models tab of the AutoML experiment, you can edit the columns if you want to show other metrics in the same overview. 
By creating a more comprehensive overview that includes various metrics, it may be easier to compare models.

To explore a model even further, you can generate explanations for each model that has been trained. When configuring an AutoML experiment, you can specify that explanations should be generated for the best performing model. 
If however, you're interested in the interpretability of another model, 
you can select the model in the overview and select Explain model.






TRACK MODEL TRAINING IN JUPYTER NOTEBOOKS WITH MLFLOW 

Configure MLflow for model tracking in notebooks

As a data scientist, you'll want to develop a model in a notebook as it allows you to quickly test and run code.

Anytime you train a model, you want the results to be reproducible. 
By tracking and logging your work, you can review your work at any time and decide what the best approach is to train a model.

MLflow is an open-source library for tracking and managing your machine learning experiments. 
In particular, MLflow Tracking is a component of MLflow that logs everything about the model you're training, 
such as parameters, metrics, and artifacts.

To use MLflow in notebooks in the Azure Machine Learning workspace, 
you'll need to install the necessary libraries and set 
Azure Machine Learning as the tracking store. 
When you've configured MLflow, you can start to use MLflow when training models in notebooks.



Configure MLflow in notebooks
You can create and edit notebooks within Azure Machine Learning or on a local device.

Use Azure Machine Learning notebooks
Within the Azure Machine Learning workspace, you can create notebooks 
and connect the notebooks to an Azure Machine Learning managed compute instance.

When you're running a notebook on a compute instance, MLflow is already configured, and ready to be used.

To verify that the necessary packages are installed, you can run the following code:

pip show mlflow
pip show azureml-mlflow

The mlflow package is the open-source library. The azureml-mlflow package contains the integration code of Azure Machine Learning with MLflow.

Use MLflow on a local device
When you prefer working in notebooks on a local device, you can also make use of MLflow. You'll need to configure MLflow by completing the following steps:

Install the mlflow and azureml-mlflow package.

pip install mlflow
pip install azureml-mlflow


Navigate to the Azure Machine Learning studio.

Select the name of the workspace you're working on in the top right corner of the studio.

Select View all properties in Azure portal. 
A new tab will open to take you to the Azure Machine Learning service in the Azure portal.

Copy the value of the MLflow tracking URI.

Use the following code in your local notebook to configure MLflow to point to the Azure Machine Learning workspace, 
and set it to the workspace tracking URI.

mlflow.set_tracking_uri = "MLFLOW-TRACKING-URI"

When you've configured MLflow to track your model's results and store it in your Azure Machine Learning workspace, 
you're ready to experiment in a notebook.



Train and track models in notebooks

As a data scientist, you use notebooks to experiment and train models. 
To group model training results, you'll use experiments. 
To track model metrics with MLflow when training a model in a notebook, you can use MLflow's logging capabilities.


Create an MLflow experiment
You can create a MLflow experiment, which allows you to group runs. 
If you don't create an experiment, MLflow will assume the default experiment with name Default.

To create an experiment, run the following command in a notebook:

import mlflow
mlflow.set_experiment(experiment_name="heart-condition-classifier")



LOG RESULTS WITH MLFLOW 


Now, you're ready to train your model. To start a run tracked by MLflow, you'll use start_run(). Next, to track the model, you can:

Enable autologging.
Use custom logging.
Enable autologging
MLflow supports automatic logging for popular machine learning libraries. If you're using a library that is supported by autolog, then MLflow tells the framework you're using to log all the metrics, parameters, artifacts, and models that the framework considers relevant.

You can turn on autologging by using the autolog method for the framework you're using. 
For example, to enable autologging for XGBoost models you can use mlflow.xgboost.autolog().

A notebook cell that trains and tracks a classification model using autologging may be similar to the following code example:


from xgboost import XGBClassifier

with mlflow.start_run():
    mlflow.xgboost.autolog()

    model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)



As soon as mlflow.xgboost.autolog() is called, MLflow will start a run within an 
experiment in Azure Machine Learning to start tracking the experiment's run.

When the job has completed, you can review all logged metrics in the studio.


Use custom logging
Additionally, you can manually log your model with MLflow. Manually logging models is helpful when you want to log supplementary or custom information that isn't logged through autologging.


Common functions used with custom logging are:

mlflow.log_param(): Logs a single key-value parameter. Use this function for an input parameter you want to log.
mlflow.log_metric(): Logs a single key-value metric. Value must be a number. 
Use this function for any output you want to store with the run.

mlflow.log_artifact(): Logs a file. Use this function for any plot you want to log, save as image file first.
mlflow.log_model(): Logs a model. Use this function to create an MLflow model, which may include a custom signature, environment, and input examples.


To use custom logging in a notebook, start a run and log any metric you want:

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

with mlflow.start_run():
    model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", accuracy)


Custom logging gives you more flexibility, but also creates more work as you'll have to 
define any parameter, metric, or artifact you want to log.

When the job has completed, you can review all logged metrics in the studio.


RESPONSIBLE AI 

### Understanding Responsible AI (Plain English)

When using machine learning models to make decisions—such as predicting loan eligibility or evaluating job candidates—it’s crucial to ensure that the models are **fair, transparent, and accountable**. This is where **Responsible AI** principles come into play. These principles help ensure that AI systems are ethical, unbiased, and trustworthy.

---

### Microsoft’s Five Responsible AI Principles:

1. **Fairness and Inclusiveness**:
   - Models should treat everyone fairly and avoid bias.
   - Similar groups should not be treated differently without justification.
   - Example: A loan approval model should not discriminate based on gender or ethnicity.

2. **Reliability and Safety**:
   - Models should be consistent, reliable, and safe to use.
   - They should handle unexpected situations gracefully and resist harmful manipulation.
   - Example: A self-driving car model should operate safely in all conditions, even in rare or unexpected scenarios.

3. **Privacy and Security**:
   - Be transparent about how data is collected, used, and stored.
   - Protect individuals' privacy and ensure their data is secure.
   - Example: A healthcare model should safeguard patient data and comply with privacy regulations.

4. **Transparency**:
   - People should understand how models make decisions, especially when those decisions affect their lives.
   - Example: A hiring model should provide clear explanations for why a candidate was or wasn’t selected.

5. **Accountability**:
   - Humans should remain in control and take responsibility for decisions influenced by AI.
   - Example: A credit approval system should allow human oversight to review and override decisions if necessary.

---

### Why Are These Principles Important?
- **Ethical AI**: Ensures that AI systems are used responsibly and do not harm individuals or groups.
- **Trust**: Builds confidence in AI systems by making them fair, transparent, and accountable.
- **Compliance**: Helps organizations meet legal and ethical standards for AI use.

---

### Summary:
Responsible AI ensures that machine learning models are ethical, unbiased, and trustworthy. By following principles like fairness, reliability, privacy, transparency, and accountability, you can create AI systems that positively impact society while minimizing risks.


CREATE RESPONSIBLE AI DASHBOARD 

### Creating the Responsible AI Dashboard (Plain English)

The **Responsible AI (RAI) dashboard** in Azure Machine Learning helps you evaluate whether your machine learning model is **safe, trustworthy, and ethical**. It provides insights into your model's behavior and allows you to share these insights with others through an interactive dashboard or a PDF scorecard.

---

### What Is the Responsible AI Dashboard?
- A tool that gathers insights about your model to ensure it aligns with **Responsible AI principles**.
- It helps you evaluate your model's fairness, reliability, transparency, and accountability.
- Insights are presented in an **interactive dashboard** for exploration or as a **scorecard** for sharing.

---

### How to Create the Responsible AI Dashboard
To create the dashboard, you need to build a **pipeline** using Azure Machine Learning's built-in components. The pipeline should:
1. **Start** with the **RAI Insights dashboard constructor**.
2. **Include** one or more RAI tool components (e.g., explanations, causal analysis, error analysis).
3. **End** with the **Gather RAI Insights dashboard** component to combine all insights into one dashboard.
4. Optionally, add the **Gather RAI Insights scorecard** component to generate a PDF summary.

---

### Available RAI Tool Components
1. **Add Explanation**:
   - Generates explanations to show how much each feature influences the model's predictions.
   - Helps interpret the model's decision-making process.

2. **Add Causal Analysis**:
   - Uses historical data to analyze the causal effects of features on outcomes.
   - Helps identify which features have the most significant impact.

3. **Add Counterfactuals**:
   - Explores how changing input values would change the model's output.
   - Useful for understanding "what-if" scenarios.

4. **Add Error Analysis**:
   - Identifies erroneous subgroups in your data and explores the distribution of errors.
   - Helps improve model performance by addressing specific issues.

---

### Steps to Build and Run the Pipeline
1. **Register Datasets**:
   - Register your training and test datasets as **MLTable data assets** in Azure Machine Learning.

2. **Register the Model**:
   - Ensure the model you want to evaluate is registered in the Azure Machine Learning workspace.

3. **Retrieve Components**:
   - Use the built-in RAI components (e.g., Explanation, Causal Analysis) to include in your pipeline.

4. **Build the Pipeline**:
   - Start with the **RAI Insights dashboard constructor**.
   - Add the desired RAI tool components.
   - End with the **Gather RAI Insights dashboard** component.

5. **Run the Pipeline**:
   - Execute the pipeline to generate the Responsible AI dashboard and scorecard.

---

### Ways to Create the Dashboard
You can create the Responsible AI dashboard using:
1. **Command Line Interface (CLI)**: For users comfortable with command-line tools.
2. **Python SDK**: For programmatic control and customization.
3. **Azure Machine Learning Studio**: A no-code, user-friendly interface.

---

### Summary
The Responsible AI dashboard helps you evaluate your model's fairness, reliability, and transparency. By building a pipeline with RAI components, you can generate insights into your model's behavior and ensure it aligns with ethical AI principles. The dashboard and scorecard make it easy to share these insights with stakeholders.

Python 
rai_constructor_component = ml_client_registry.components.get(
    name="microsoft_azureml_rai_tabular_insight_constructor", label="latest"
)

Then, you can add any of the available insights, like the explanations, by retrieving the Add Explanation to RAI Insights dashboard component:

rai_explanation_component = ml_client_registry.components.get(
    name="microsoft_azureml_rai_tabular_explanation", label="latest"
)


And finally, your pipeline should end with a Gather RAI Insights dashboard component:

rai_gather_component = ml_client_registry.components.get(
    name="microsoft_azureml_rai_tabular_insight_gather", label="latest"
)



Once you have the components, you can build the pipeline:

from azure.ai.ml import Input, dsl
from azure.ai.ml.constants import AssetTypes

@dsl.pipeline(
    compute="aml-cluster",
    experiment_name="Create RAI Dashboard",
)
def rai_decision_pipeline(
    target_column_name, train_data, test_data
):
    # Initiate the RAIInsights
    create_rai_job = rai_constructor_component(
        title="RAI dashboard diabetes",
        task_type="classification",
        model_info=expected_model_id,
        model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),
        train_dataset=train_data,
        test_dataset=test_data,
        target_column_name="Predictions",
    )
    create_rai_job.set_limits(timeout=30)

    # Add explanations
    explanation_job = rai_explanation_component(
        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,
        comment="add explanation", 
    )
    explanation_job.set_limits(timeout=10)

    # Combine everything
    rai_gather_job = rai_gather_component(
        constructor=create_rai_job.outputs.rai_insights_dashboard,
        insight=explanation_job.outputs.explanation,
    )
    rai_gather_job.set_limits(timeout=10)

    rai_gather_job.outputs.dashboard.mode = "upload"

    return {
        "dashboard": rai_gather_job.outputs.dashboard,
    }

### Explanation of the Code in Plain English

This code defines a **pipeline** in Azure Machine Learning to create a **Responsible AI (RAI) dashboard**. The pipeline gathers insights about a machine learning model, such as explanations of predictions, and combines them into an interactive dashboard.

---

### Step-by-Step Explanation:

1. **Import Required Libraries**:
   - `Input` and `dsl`: Used to define the pipeline and its components.
   - `AssetTypes`: Specifies the type of assets (e.g., MLflow models, datasets) used in the pipeline.

2. **Define the Pipeline**:
   - The `@dsl.pipeline` decorator defines a pipeline named `rai_decision_pipeline`.
   - The pipeline runs on the compute cluster `"aml-cluster"` and is part of the experiment `"Create RAI Dashboard"`.

3. **Pipeline Inputs**:
   - The pipeline accepts three inputs:
     - `target_column_name`: The name of the column containing the target variable (e.g., predictions).
     - `train_data`: The training dataset.
     - `test_data`: The test dataset.

4. **Step 1: Create the RAI Insights Dashboard**:
   - The `rai_constructor_component` initializes the RAI dashboard.
   - Key parameters:
     - **`title`**: The title of the dashboard (e.g., `"RAI dashboard diabetes"`).
     - **`task_type`**: Specifies the type of task (e.g., `"classification"`).
     - **`model_input`**: Points to the registered MLflow model.
     - **`train_dataset` and `test_dataset`**: The training and test datasets.
     - **`target_column_name`**: Specifies the column containing the model's predictions.
   - The `set_limits(timeout=30)` method sets a timeout of 30 minutes for this step.

5. **Step 2: Add Explanations**:
   - The `rai_explanation_component` generates explanations for the model's predictions.
   - Key parameters:
     - **`rai_insights_dashboard`**: Links this step to the RAI dashboard created in Step 1.
     - **`comment`**: Adds a comment (e.g., `"add explanation"`).
   - The `set_limits(timeout=10)` method sets a timeout of 10 minutes for this step.

6. **Step 3: Combine Insights into the Dashboard**:
   - The `rai_gather_component` combines the RAI dashboard and the generated explanations into a single dashboard.
   - Key parameters:
     - **`constructor`**: Links to the RAI dashboard created in Step 1.
     - **`insight`**: Links to the explanations generated in Step 2.
   - The `set_limits(timeout=10)` method sets a timeout of 10 minutes for this step.
   - The `rai_gather_job.outputs.dashboard.mode = "upload"` ensures the dashboard is uploaded to Azure Machine Learning.

7. **Pipeline Output**:
   - The pipeline returns the final **dashboard** as its output, which can be explored interactively.

---

### What Does This Pipeline Do?
- It creates a **Responsible AI dashboard** for a classification model.
- The dashboard includes insights such as explanations of predictions.
- The pipeline combines these insights into a single interactive dashboard that can be uploaded and explored in Azure Machine Learning.

---

### Why Use This Pipeline?
- **Automation**: Automates the creation of a Responsible AI dashboard.
- **Transparency**: Provides insights into how the model makes predictions.
- **Compliance**: Helps ensure the model aligns with Responsible AI principles like fairness and accountability.

In short, this pipeline simplifies the process of generating and exploring Responsible AI insights for a machine learning model.


EVALUATE RESPONSIBLE AI DASHBOARD 
### Evaluating the Responsible AI Dashboard (Plain English)

The **Responsible AI (RAI) dashboard** in Azure Machine Learning helps you evaluate your machine learning model by providing insights into its behavior. Once the dashboard is generated, you can explore its contents in the Azure Machine Learning studio to better understand and improve your model.

---

### How to Access the Dashboard
- Open the Responsible AI dashboard in the Azure Machine Learning studio.
- The studio automatically connects the dashboard to a compute instance, enabling interactive exploration.

---

### Insights Available in the Dashboard
The dashboard reflects the outputs of the components you included in the pipeline. These insights may include:

1. **Error Analysis**:
   - Helps you understand how errors are distributed in your dataset.
   - **Visuals**:
     - **Error Tree Map**: Shows combinations of subgroups where the model makes more false predictions.
     - **Error Heat Map**: Displays a grid overview of errors across one or two features.

2. **Explanations**:
   - Explains how the model makes predictions by calculating **feature importance**.
   - **Types of Feature Importance**:
     - **Aggregate Feature Importance**: Shows how each feature influences predictions across the entire dataset.
     - **Individual Feature Importance**: Explains how each feature impacts a specific prediction.

3. **Counterfactuals**:
   - Explores "what-if" scenarios to see how changes in input features would affect the model's predictions.
   - **Example**: Select a data point and specify a desired prediction to see what input changes would lead to that outcome.

4. **Causal Analysis**:
   - Estimates the causal effects of features on predictions to improve decision-making.
   - **Tabs in the Dashboard**:
     - **Aggregate Causal Effects**: Shows the average effect of changing specific features on predictions.
     - **Individual Causal Effects**: Allows you to explore how changing features for individual data points affects predictions.
     - **Treatment Policy**: Identifies which data points benefit most from specific changes to features.

---

### Why Use the Responsible AI Dashboard?
1. **Error Analysis**: Identifies subgroups where the model performs poorly, helping you target improvements.
2. **Explanations**: Provides transparency into how the model makes decisions, building trust and interpretability.
3. **Counterfactuals**: Helps you explore alternative scenarios to understand how predictions can change.
4. **Causal Analysis**: Offers actionable insights into how feature changes can optimize outcomes.

---

### Summary
The Responsible AI dashboard provides a comprehensive view of your model's behavior, helping you evaluate its fairness, reliability, and transparency. By exploring insights like error analysis, explanations, counterfactuals, and causal analysis, you can better understand your model and make informed decisions to improve its performance and alignment with Responsible AI principles.



