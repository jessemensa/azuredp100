- RUN TRAINING SCRIPT AS A COMMAND JOB IN AZURE MACHINE LEARNING 


Convert a notebook to a script

When you've used notebooks for experimentation and development, you'll first need to convert a notebook to a script. Alternatively, you might choose to skip using notebooks and work only with scripts. 
Either way, there are some recommendations when creating scripts to have production-ready code.

Scripts are ideal for testing and automation in your production environment. 
To create a production-ready script, you'll need to:

Remove nonessential code.
Refactor your code into functions.
Test your script in the terminal.


Remove all nonessential code
The main benefit of using notebooks is being able to quickly explore your data. 
For example, you can use print() and df.describe() statements to explore your data and variables. 
When you create a script that will be used for automation, 
you want to avoid including code written for exploratory purposes.

The first thing you therefore need to do to convert your code to production code is to remove the nonessential code. 
Especially when you'll run the code regularly, you want to avoid executing anything nonessential 
to reduce cost and compute time.


Refactor your code into functions
When using code in business processes, you want the code to be easy to read so that anyone can maintain it. 
One common approach to make code easier to read and test is to use functions.

For example, you might have used the following example code in a notebook to read and split the data:

# read and visualize the data
print("Reading data...")
df = pd.read_csv('diabetes.csv')
df.head()

# split data
print("Splitting data...")
X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)


As functions also allow you to test parts of your code, you might prefer to create multiple smaller functions, rather than one large function. If you want to test a part of your code, you can choose to only test a small part and avoid running more code than necessary.

You can refactor the code shown in the example into two functions:

Read the data
Split the data
An example of refactored code might be the following:


def main(csv_file):
    # read data
    df = get_data(csv_file)

    # split data
    X_train, X_test, y_train, y_test = split_data(df)

# function that reads the data
def get_data(path):
    df = pd.read_csv(path)
    
    return df

# function that splits the data
def split_data(df):
    X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',
    'SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

    return X_train, X_test, y_train, y_test


Test your script
Before using scripts in production environments, for example by integrating them with automation pipelines, 
you'll want to test whether the scripts work as expected.

One simple way to test your script, is to run the script in a terminal. 
Within the Azure Machine Learning workspace, you can quickly run a script in the terminal of the compute instance.

When you open a script in the Notebooks page of the Azure Machine Learning studio, 
you can choose to save and run the script in the terminal.

Alternatively, you can navigate directly to the terminal of the compute instance. 
Navigate to the Compute page and select the Terminal of the compute instance you want to use. 
You can use the following command to run a Python script named train.py:

-> python train.py



RUN A SCRIPT AS A COMMAND JOB 

When you have a script that train a machine learning model, you can run it as a command job in Azure Machine Learning.

Configure and submit a command job
To run a script as a command job, you'll need to configure and submit the job.

To configure a command job with the Python SDK (v2), you'll use the command function. To run a script, you'll need to specify values for the following parameters:

code: The folder that includes the script to run.
command: Specifies which file to run.
environment: The necessary packages to be installed on the compute before running the command.
compute: The compute to use to run the command.
display_name: The name of the individual job.
experiment_name: The name of the experiment the job belongs to.


You can configure a command job to run a file named train.py, on the compute cluster named aml-cluster with the following code:

from azure.ai.ml import command

# configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )

When your job is configured, you can submit it, which will initiate the job and run the script:

# submit job
returned_job = ml_client.create_or_update(job)


You can monitor and review the job in the Azure Machine Learning studio. 
All jobs with the same experiment name will be grouped under the same experiment. 
You can find an individual job using the specified display name.

All inputs and outputs of a command job are tracked. You can review which command you specified, 
which compute was used, and which environment was used to run the script on the specified compute.



USE PARAMETERS IN A COMMAND JOB 


You can increase the flexibility of your scripts by using parameters. 
For example, you might have created a script that trains a machine learning model. 
You can use the same script to train a model on different datasets, or using various hyperparameter values.

# import libraries
import argparse
import pandas as pd
from sklearn.linear_model import LogisticRegression

def main(args):
    # read data
    df = get_data(args.training_data)

# function that reads the data
def get_data(path):
    df = pd.read_csv(path)
    
    return df

def parse_args():
    # setup arg parser
    parser = argparse.ArgumentParser()

    # add arguments
    parser.add_argument("--training_data", dest='training_data',
                        type=str)

    # parse args
    args = parser.parse_args()

    # return args
    return args

# run script
if __name__ == "__main__":

    # parse args
    args = parse_args()

    # run main function
    main(args)



Any parameters you expect should be defined in the script. 
In the script, you can specify what type of value you expect for each parameter and whether you want to set a default value.

Passing arguments to a script
To pass parameter values to a script, you need to provide the argument value in the command.

For example, if you would pass a parameter value when running a script in a terminal, you would use the following command:

python train.py --training_data diabetes.csv



In the example, diabetes.csv is a local file. Alternatively, you could specify the 
path to a data asset created in the Azure Machine Learning workspace.

Similarly, when you want to pass a parameter value to a script you want to run as a command job, 
you'll specify the values in the command:

from azure.ai.ml import command

# configure job
job = command(
    code="./src",
    command="python train.py --training_data diabetes.csv",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="train-model",
    experiment_name="train-classification-model"
    )





TRACK MODEL WITH MLFLOW 

When you train a model with a script, you can include MLflow in the scripts to track any parameters, metrics, and artifacts. When you run the script as a job in Azure Machine Learning, 
you're able to review all input parameters and outputs for each run.

Understand MLflow
MLflow is an open-source platform, designed to manage the complete machine learning lifecycle. 
As it's open source, it can be used when training models on different platforms. 
Here, we explore how we can integrate MLflow with Azure Machine Learning jobs.

There are two options to track machine learning jobs with MLflow:

Enable autologging using mlflow.autolog()
Use logging functions to track custom metrics using mlflow.log_*
Before you can use either of these options, you need to set up the environment to use MLflow.

Include MLflow in the environment
To use MLflow during training job, the mlflow and azureml-mlflow pip 
packages need to be installed on the compute executing the script. 
Therefore, you need to include these two packages in the environment. 
You can create an environment by referring to a YAML file that describes the Conda environment. As part of the Conda environment, you can include these two packages.

For example, in this custom environment mlflow and azureml-mlflow are installed using pip:

name: mlflow-env
channels:
  - conda-forge
dependencies:
  - python=3.8
  - pip
  - pip:
    - numpy
    - pandas
    - scikit-learn
    - matplotlib
    - mlflow
    - azureml-mlflow

Once the environment is defined and registered, make sure to refer to it when submitting a job.

Enable autologging
When working with one of the common libraries for machine learning, you can enable autologging in MLflow. 
Autologging logs parameters, metrics, and model artifacts without anyone needing to specify what needs to be logged.

Autologging is supported for the following libraries:

Scikit-learn
TensorFlow and Keras
XGBoost
LightGBM
Spark
Fastai
Pytorch
To enable autologging, add the following code to your training script:

import mlflow

mlflow.autolog()



Log metrics with MLflow
In your training script, you can decide whatever custom metric you want to log with MLflow.

Depending on the type of value you want to log, use the MLflow command to store the metric with the experiment run:

mlflow.log_param(): Log single key-value parameter. Use this function for an input parameter you want to log.
mlflow.log_metric(): Log single key-value metric. Value must be a number. Use this function for any output you want to store with the run.
mlflow.log_artifact(): Log a file. Use this function for any plot you want to log, save as image file first.
To add MLflow to an existing training script, you can add the following code:

import mlflow

reg_rate = 0.1
mlflow.log_param("Regularization rate", reg_rate)


Submit the job
Finally, you need to submit the training script as a job in Azure Machine Learning. 
When you use MLflow in a training script and run it as a job, all tracked parameters, 
metrics, and artifacts are stored with the job run.

You configure the job as usual. You only need to make sure that the environment you 
refer to in the job includes the necessary packages, and the script describes which metrics you want to log.



VIEW METRICS AND EVALUATE MODELS 
After you've trained and tracked models with MLflow in Azure Machine Learning, 
you can explore the metrics and evaluate your models.

Review metrics in the Azure Machine Learning studio.
Retrieve runs and metrics with MLflow.


View the metrics in the Azure Machine Learning studio
When your job is completed, you can review the logged parameters, metrics, and artifacts in the Azure Machine Learning studio.

When you review job runs in the Azure Machine Learning studio, you'll explore a job run's metrics, which is part of an experiment.

To view the metrics through an intuitive user interface, you can:

Open the Studio by navigating to https://ml.azure.com.
Find your experiment run and open it to view its details.
In the Details tab, all logged parameters are shown under Params.
Select the Metrics tab and select the metric you want to explore.
Any plots that are logged as artifacts can be found under Images.
The model assets that can be used to register and deploy the model are stored in the models folder under Outputs + logs.


Retrieve metrics with MLflow in a notebook
When you run a training script as a job in Azure Machine Learning, and track your model training with MLflow, you can query the runs in a notebook by using MLflow. Using MLflow in a notebook gives you more control over which runs you want to retrieve to compare.

When using MLflow to query your runs, you'll refer to experiments and runs.

Search all the experiments
You can get all the active experiments in the workspace using MLFlow:


experiments = mlflow.search_experiments(max_results=2)
for exp in experiments:
    print(exp.name)

If you want to retrieve archived experiments too, then include the option ViewType.ALL:
from mlflow.entities import ViewType

experiments = mlflow.search_experiments(view_type=ViewType.ALL)
for exp in experiments:
    print(exp.name)


To retrieve a specific experiment, you can run:

exp = mlflow.get_experiment_by_name(experiment_name)
print(exp)


Retrieve runs
MLflow allows you to search for runs inside of any experiment. 
You need either the experiment ID or the experiment name.

For example, when you want to retrieve the metrics of a specific run:

mlflow.search_runs(exp.experiment_id)


You can search runs across multiple experiments if necessary. Searching across experiments may be useful in case you want to compare runs of the same model when it's being logged in different experiments (by different people or different project iterations).

You can use search_all_experiments=True if you want to search across all the experiments in the workspace.

By default, experiments are ordered descending by start_time, which is the time the experiment was queued in Azure Machine Learning. However, you can change this default by using the parameter order_by.

For example, if you want to sort by start time and only show the last two results:


mlflow.search_runs(exp.experiment_id, order_by=["start_time DESC"], max_results=2)


You can also look for a run with a specific combination in the hyperparameters:


mlflow.search_runs(
    exp.experiment_id, filter_string="params.num_boost_round='100'", max_results=2
)


PERFORM HYPERPARAMETER TUNING WITH AZURE MACHINE LEARNING 

In machine learning, models are trained to predict unknown labels for new data based on correlations between known labels and features found in the training data. 
Depending on the algorithm used, you may need to specify hyperparameters to configure how the model is trained.

For example, the logistic regression algorithm uses a regularization rate hyperparameter to counteract overfitting; and deep learning techniques for convolutional neural networks (CNNs) use hyperparameters like learning rate to control how weights are adjusted during training, and batch size to determine how many data items are included in each training batch.


The choice of hyperparameter values can significantly affect the resulting model, making it important to select the best possible values for your particular data and predictive performance goals.



Tuning hyperparameters


Hyperparameter tuning is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. The resulting model from each training run is then evaluated to determine the performance metric for which you want to optimize (for example, accuracy), and the best-performing model is selected.

In Azure Machine Learning, you can tune hyperparameters by submitting a script as a sweep job. A sweep job will run a trial for each hyperparameter combination to be tested. Each trial uses a training script with parameterized hyperparameter values to train a model, and logs the target performance metric achieved by the trained model.



DEFINE SEARCH SPACE 

The set of hyperparameter values tried during hyperparameter tuning is known as the search space. 
The definition of the range of possible values that can be chosen depends on the type of hyperparameter.


Discrete hyperparameters
Some hyperparameters require discrete values - in other words, you must select the value from a particular finite set of possibilities. You can define a search space for a discrete parameter using a Choice from a list of explicit values, which you can define as a Python list (Choice(values=[10,20,30])), a range (Choice(values=range(1,10))), or an arbitrary set of comma-separated values (Choice(values=(30,50,100)))

You can also select discrete values from any of the following discrete distributions:

QUniform(min_value, max_value, q): Returns a value like round(Uniform(min_value, max_value) / q) * q
QLogUniform(min_value, max_value, q): Returns a value like round(exp(Uniform(min_value, max_value)) / q) * q
QNormal(mu, sigma, q): Returns a value like round(Normal(mu, sigma) / q) * q
QLogNormal(mu, sigma, q): Returns a value like round(exp(Normal(mu, sigma)) / q) * q




Continuous hyperparameters
Some hyperparameters are continuous - in other words you can use any value along a scale, resulting in an infinite number of possibilities. To define a search space for these kinds of value, you can use any of the following distribution types:

Uniform(min_value, max_value): Returns a value uniformly distributed between min_value and max_value
LogUniform(min_value, max_value): Returns a value drawn according to exp(Uniform(min_value, max_value)) 
so that the logarithm of the return value is uniformly distributed
Normal(mu, sigma): Returns a real value that's normally distributed with mean mu and standard deviation sigma
LogNormal(mu, sigma): Returns a value drawn according to exp(Normal(mu, sigma)) so that the logarithm of the return value is normally distributed



Defining a search space
To define a search space for hyperparameter tuning, create a dictionary with the 
appropriate parameter expression for each named hyperparameter.

For example, the following search space indicates that the batch_size hyperparameter can have the 
value 16, 32, or 64, and the learning_rate hyperparameter can have any value from a normal distribution 
with a mean of 10 and a standard deviation of 3.

from azure.ai.ml.sweep import Choice, Normal

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),    
    learning_rate=Normal(mu=10, sigma=3),
)


CONFIGURE A SAMPLING METHOD 

The specific values used in a hyperparameter tuning run, or sweep job, depend on the type of sampling used.

There are three main sampling methods available in Azure Machine Learning:

Grid sampling: Tries every possible combination.
Random sampling: Randomly chooses values from the search space.
Sobol: Adds a seed to random sampling to make the results reproducible.
Bayesian sampling: Chooses new values based on previous results.


Grid sampling
Grid sampling can only be applied when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space.

For example, in the following code example, grid sampling is used to try 
every possible combination of discrete batch_size and learning_rate value:

from azure.ai.ml.sweep import Choice

command_job_for_sweep = command_job(
    batch_size=Choice(values=[16, 32, 64]),
    learning_rate=Choice(values=[0.01, 0.1, 1.0]),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "grid",
    ...
)



Random sampling
Random sampling is used to randomly select a value for each hyperparameter, 
which can be a mix of discrete and continuous values as shown in the following code example:

from azure.ai.ml.sweep import Normal, Uniform

command_job_for_sweep = command_job(
    batch_size=Choice(values=[16, 32, 64]),   
    learning_rate=Normal(mu=10, sigma=3),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "random",
    ...
)


Sobol
You may want to be able to reproduce a random sampling sweep job. If you expect that you do, you can use Sobol instead. 
Sobol is a type of random sampling that allows you to use a seed. When you add a seed, the sweep job can be reproduced, 
and the search space distribution is spread more evenly.

The following code example shows how to use Sobol by adding a seed and a rule, and using the RandomSamplingAlgorithm class:

from azure.ai.ml.sweep import RandomSamplingAlgorithm

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = RandomSamplingAlgorithm(seed=123, rule="sobol"),
    ...
)


Bayesian sampling
Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection. The following code example shows how to configure Bayesian sampling:


from azure.ai.ml.sweep import Uniform, Choice

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),    
    learning_rate=Uniform(min_value=0.05, max_value=0.1),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "bayesian",
    ...
)


CONFIGURE EARLY TERMINATION 

Hyperparameter tuning helps you fine-tune your model and select the hyperparameter values that will make your model perform best.

For you to find the best model, however, can be a never-ending conquest. You always have to consider 
whether it's worth the time and expense of testing new hyperparameter values to find a model that may perform better.

Each trial in a sweep job, a new model is trained with a new combination of hyperparameter values. 
If training a new model doesn't result in a significantly better model, you may want to stop the sweep job and use the model that performed best so far.

When you configure a sweep job in Azure Machine Learning, you can also set a maximum number of trials. 
A more sophisticated approach may be to stop a sweep job when newer models don't produce significantly better results. 
To stop a sweep job based on the performance of the models, you can use an early termination policy.


When to use an early termination policy
Whether you want to use an early termination policy may depend on the search space and sampling method you're working with.

For example, you may choose to use a grid sampling method over a discrete search space that results in a maximum of six trials. With six trials, a maximum of six models will be trained and an early termination policy may be unnecessary.

An early termination policy can be especially beneficial when working with continuous hyperparameters in your search space. Continuous hyperparameters present an unlimited number of possible values to choose from. You'll most likely want to use an early termination policy when working with continuous hyperparameters and a random or Bayesian sampling method.



Configure an early termination policy
There are two main parameters when you choose to use an early termination policy:

evaluation_interval: Specifies at which interval you want the policy to be evaluated. Every time the primary metric is logged for a trial counts as an interval.
delay_evaluation: Specifies when to start evaluating the policy. This parameter allows for at least a minimum of trials to complete without an early termination policy affecting them.
New models may continue to perform only slightly better than previous models. To determine the extent to which a model should perform better than previous trials, there are three options for early termination:

Bandit policy: Uses a slack_factor (relative) or slack_amount(absolute). 
Any new model must perform within the slack range of the best performing model.
Median stopping policy: Uses the median of the averages of the primary metric. 
Any new model must perform better than the median.
Truncation selection policy: Uses a truncation_percentage, which is the percentage of lowest performing trials. 
Any new model must perform better than the lowest performing trials.



Bandit policy
You can use a bandit policy to stop a trial if the target performance metric underperforms 
the best trial so far by a specified margin.

For example, the following code applies a bandit policy with a delay of five trials, 
evaluates the policy at every interval, and allows an absolute slack amount of 0.2.

PYTHON 
from azure.ai.ml.sweep import BanditPolicy

sweep_job.early_termination = BanditPolicy(
    slack_amount = 0.2, 
    delay_evaluation = 5, 
    evaluation_interval = 1
)

Imagine the primary metric is the accuracy of the model. When after the first five trials, the best performing model has an accuracy of 0.9, any new model needs to perform better than (0.9-0.2) or 0.7. If the new model's accuracy is higher than 0.7, the sweep job will continue. If the new model has an accuracy score lower than 0.7, the policy will terminate the sweep job.


You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio rather than an absolute value.

Median stopping policy
A median stopping policy abandons trials where the target performance metric is worse than the median of the running averages for all trials.

For example, the following code applies a median stopping policy with a delay of five trials and evaluates the policy at every interval.

from azure.ai.ml.sweep import MedianStoppingPolicy

sweep_job.early_termination = MedianStoppingPolicy(
    delay_evaluation = 5, 
    evaluation_interval = 1
)


Imagine the primary metric is the accuracy of the model. When the accuracy is logged for the sixth trial, the metric needs to be higher than the median of the accuracy scores so far. Suppose the median of the accuracy scores so far is 0.82. If the new model's accuracy is higher than 0.82, the sweep job will continue. If the new model has an accuracy score lower than 0.82, the policy will stop the sweep job, and no new models will be trained.


Truncation selection policy
A truncation selection policy cancels the lowest performing X% of trials at each evaluation interval based on the truncation_percentage value you specify for X.

For example, the following code applies a truncation selection policy with a delay of four trials, evaluates the policy at every interval, and uses a truncation percentage of 20%.

from azure.ai.ml.sweep import TruncationSelectionPolicy

sweep_job.early_termination = TruncationSelectionPolicy(
    evaluation_interval=1, 
    truncation_percentage=20, 
    delay_evaluation=4 
)


Imagine the primary metric is the accuracy of the model. When the accuracy is logged for the fifth trial, the metric should not be in the worst 20% of the trials so far. In this case, 20% translates to one trial. In other words, if the fifth trial is not the worst performing model so far, the sweep job will continue. If the fifth trial has the lowest accuracy score of all trials so far, the sweep job will stop.


USE A SWEEP JOB FOR HYPERPARAMETER TUNING 

Create a training script for hyperparameter tuning
To run a sweep job, you need to create a training script just the way you would do for any other training job, 
except that your script must:

Include an argument for each hyperparameter you want to vary.
Log the target performance metric with MLflow. A logged metric enables the sweep job to evaluate 
the performance of the trials it initiates, and identify the one that produces the best performing model.

For example, the following example script trains a logistic regression model using a --regularization argument to set the regularization rate hyperparameter, and logs the accuracy metric with the name Accuracy:

import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import mlflow

# get regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)
args = parser.parse_args()
reg = args.reg_rate

# load the training dataset
data = pd.read_csv("data.csv")

# separate features and labels, and split for training/validatiom
X = data[['feature1','feature2','feature3','feature4']].values
y = data['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# train a logistic regression model with the reg hyperparameter
model = LogisticRegression(C=1/reg, solver="liblinear").fit(X_train, y_train)

# calculate and log accuracy
y_hat = model.predict(X_test)
acc = np.average(y_hat == y_test)
mlflow.log_metric("Accuracy", acc)

Configure and run a sweep job
To prepare the sweep job, you must first create a base command job that specifies which script to run and defines the parameters used by the script:

from azure.ai.ml import command

# configure command job as base
job = command(
    code="./src",
    command="python train.py --regularization ${{inputs.reg_rate}}",
    inputs={
        "reg_rate": 0.01,
    },
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    )

You can then override your input parameters with your search space:

from azure.ai.ml.sweep import Choice

command_job_for_sweep = job(
    reg_rate=Choice(values=[0.01, 0.1, 1]),
)


Finally, call sweep() on your command job to sweep over your search space:

from azure.ai.ml import MLClient

# apply the sweep parameter to obtain the sweep_job
sweep_job = command_job_for_sweep.sweep(
    compute="aml-cluster",
    sampling_algorithm="grid",
    primary_metric="Accuracy",
    goal="Maximize",
)

# set the name of the sweep job experiment
sweep_job.experiment_name="sweep-example"

# define the limits for this sweep
sweep_job.set_limits(max_total_trials=4, max_concurrent_trials=2, timeout=7200)

# submit the sweep
returned_sweep_job = ml_client.create_or_update(sweep_job)


Monitor and review sweep jobs
You can monitor sweep jobs in Azure Machine Learning studio. The sweep job will initiate trials for each hyperparameter combination to be tried. For each trial, you can review all logged metrics.

Additionally, you can evaluate and compare models by visualizing the trials in the studio. 
You can adjust each chart to show and compare the hyperparameter values and metrics for each trial.


RUN PIPELINES IN AZURE MACHINE LEARNING 

Create components

Components allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace. You can also use components to build an Azure Machine Learning pipeline.

Use a component
There are two main reasons why you'd use components:

To build a pipeline.
To share ready-to-go code.
You'll want to create components when you're preparing your code for scale. When you're done with experimenting and developing, and ready to move your model to production.

Within Azure Machine Learning, you can create a component to store code (in your preferred language) within the workspace. Ideally, you design a component to perform a specific action that is relevant to your machine learning workflow.

For example, a component may consist of a Python script that normalizes your data, trains a machine learning model, or evaluates a model.

Components can be easily shared to other Azure Machine Learning users, who can reuse components in their own Azure Machine Learning pipelines.


Create a component
A component consists of three parts:

Metadata: Includes the component's name, version, etc.
Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts).
Command, code and environment: Specifies how to run the code.
To create a component, you need two files:

A script that contains the workflow you want to execute.
A YAML file to define the metadata, interface, and command, code, and environment of the component.
You can create the YAML file, or use the command_component() function as a decorator to create the YAML file.#

For example, you may have a Python script prep.py that prepares the data by removing missing values and normalizing the data:

# import libraries
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler

# setup arg parser
parser = argparse.ArgumentParser()

# add arguments
parser.add_argument("--input_data", dest='input_data',
                    type=str)
parser.add_argument("--output_data", dest='output_data',
                    type=str)

# parse args
args = parser.parse_args()

# read the data
df = pd.read_csv(args.input_data)

# remove missing values
df = df.dropna()

# normalize the data    
scaler = MinMaxScaler()
num_cols = ['feature1','feature2','feature3','feature4']
df[num_cols] = scaler.fit_transform(df[num_cols])

# save the data as a csv
output_df = df.to_csv(
    (Path(args.output_data) / "prepped-data.csv"), 
    index = False
)


To create a component for the prep.py script, you'll need a YAML file prep.yml:


$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
name: prep_data
display_name: Prepare training data
version: 1
type: command
inputs:
  input_data: 
    type: uri_file
outputs:
  output_data:
    type: uri_file
code: ./src
environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest
command: >-
  python prep.py 
  --input_data ${{inputs.input_data}}
  --output_data ${{outputs.output_data}}


Notice that the YAML file refers to the prep.py script, which is stored in the src folder. You can load the component with the following code:

from azure.ai.ml import load_component
parent_dir = ""

loaded_component_prep = load_component(source=parent_dir + "./prep.yml")


When you've loaded the component, you can use it in a pipeline or register the component.

Register a component
To use components in a pipeline, you'll need the script and the YAML file. 
To make the components accessible to other users in the workspace, 
you can also register components to the Azure Machine Learning workspace.

You can register a component with the following code:

prep = ml_client.components.create_or_update(prepare_data_component)



CREATE A PIPELINE 

In Azure Machine Learning, a pipeline is a workflow of machine learning tasks in which each task is defined as a component.

Components can be arranged sequentially or in parallel, enabling you to build sophisticated flow logic to orchestrate machine learning operations. Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal.

A pipeline can be executed as a process by running the pipeline as a pipeline job. Each component is executed as a child job as part of the overall pipeline job.

Build a pipeline
An Azure Machine Learning pipeline is defined in a YAML file. The YAML file includes the pipeline job name, inputs, outputs, and settings.

You can create the YAML file, or use the @pipeline() function to create the YAML file.


For example, if you want to build a pipeline that first prepares the data, and then trains the model, you can use the following code:

from azure.ai.ml.dsl import pipeline

@pipeline()
def pipeline_function_name(pipeline_job_input):
    prep_data = loaded_component_prep(input_data=pipeline_job_input)
    train_model = loaded_component_train(training_data=prep_data.outputs.output_data)

    return {
        "pipeline_job_transformed_data": prep_data.outputs.output_data,
        "pipeline_job_trained_model": train_model.outputs.model_output,
    }

To pass a registered data asset as the pipeline job input, you can call the function you created with the data asset as input:


from azure.ai.ml.dsl import pipeline

@pipeline()
def pipeline_function_name(pipeline_job_input):
    prep_data = loaded_component_prep(input_data=pipeline_job_input)
    train_model = loaded_component_train(training_data=prep_data.outputs.output_data)

    return {
        "pipeline_job_transformed_data": prep_data.outputs.output_data,
        "pipeline_job_trained_model": train_model.outputs.model_output,
    }

To pass a registered data asset as the pipeline job input, you can call the function you created with the data asset as input:

from azure.ai.ml import Input
from azure.ai.ml.constants import AssetTypes

pipeline_job = pipeline_function_name(
    Input(type=AssetTypes.URI_FILE, 
    path="azureml:data:1"
))


The @pipeline() function builds a pipeline consisting of two sequential steps, represented by the two loaded components.

To understand the pipeline built in the example, let's explore it step by step:

The pipeline is built by defining the function pipeline_function_name.
The pipeline function expects pipeline_job_input as the overall pipeline input.
The first pipeline step requires a value for the input parameter input_data. The value for the input will be the value of pipeline_job_input.
The first pipeline step is defined by the loaded component for prep_data.
The value of the output_data of the first pipeline step is used for the expected input training_data of the second pipeline step.
The second pipeline step is defined by the loaded component for train_model and results in a trained model referred to by model_output.
Pipeline outputs are defined by returning variables from the pipeline function. There are two outputs:
pipeline_job_transformed_data with the value of prep_data.outputs.output_data
pipeline_job_trained_model with the value of train_model.outputs.model_output



The result of running the @pipeline() function is a YAML file that you can review by printing the pipeline_job object you created when calling the function:

print(pipeline_job)

The output will be formatted as a YAML file, which includes the configuration of the pipeline and its components. Some parameters included in the YAML file are shown in the following example.

display_name: pipeline_function_name
type: pipeline
inputs:
  pipeline_job_input:
    type: uri_file
    path: azureml:data:1
outputs:
  pipeline_job_transformed_data: null
  pipeline_job_trained_model: null
jobs:
  prep_data:
    type: command
    inputs:
      input_data:
        path: ${{parent.inputs.pipeline_job_input}}
    outputs:
      output_data: ${{parent.outputs.pipeline_job_transformed_data}}
  train_model:
    type: command
    inputs:
      input_data:
        path: ${{parent.outputs.pipeline_job_transformed_data}}
    outputs:
      output_model: ${{parent.outputs.pipeline_job_trained_model}}
tags: {}
properties: {}
settings: {}


RUN A PIPELINE JOB 
When you've built a component-based pipeline in Azure Machine Learning, you can run the workflow as a pipeline job.

Configure a pipeline job
A pipeline is defined in a YAML file, which you can also create using the @pipeline() function. After you've used the function, you can edit the pipeline configurations by specifying which parameters you want to change and the new value.

For example, you may want to change the output mode for the pipeline job outputs:

# change the output mode
pipeline_job.outputs.pipeline_job_transformed_data.mode = "upload"
pipeline_job.outputs.pipeline_job_trained_model.mode = "upload"

Or, you may want to set the default pipeline compute. When a compute isn't specified for a component, it will use the default compute instead:

# set pipeline level compute
pipeline_job.settings.default_compute = "aml-cluster"

You may also want to change the default datastore to where all outputs will be stored:
# set pipeline level datastore
pipeline_job.settings.default_datastore = "workspaceblobstore"


To review your pipeline configuration, you can print the pipeline job object:

print(pipeline_job)


Run a pipeline job
When you've configured the pipeline, you're ready to run the workflow as a pipeline job.

To submit the pipeline job, run the following code:

# submit job to workspace
pipeline_job = ml_client.jobs.create_or_update(
    pipeline_job, experiment_name="pipeline_job"
)

After you submit a pipeline job, a new job will be created in the Azure Machine Learning workspace. A pipeline job also contains child jobs, which represent the execution of the individual components. The Azure Machine Learning studio creates a graphical representation of your pipeline. You can expand the Job overview to explore the pipeline parameters, outputs, and child jobs:


To troubleshoot a failed pipeline, you can check the outputs and logs of the pipeline job and its child jobs.

If there's an issue with the configuration of the pipeline itself, 
you'll find more information in the outputs and logs of the pipeline job.
If there's an issue with the configuration of a component, 
you'll find more information in the outputs and logs of the child job of the failed component.



Schedule a pipeline job
A pipeline is ideal if you want to get your model ready for production. Pipelines are especially useful for automating the retraining of a machine learning model. To automate the retraining of a model, you can schedule a pipeline.

To schedule a pipeline job, you'll use the JobSchedule class to associate a schedule to a pipeline job.

There are various ways to create a schedule. A simple approach is to create a time-based schedule using the RecurrenceTrigger class with the following parameters:

frequency: Unit of time to describe how often the schedule fires. Value can be either minute, hour, day, week, or month.
interval: Number of frequency units to describe how often the schedule fires. Value needs to be an integer.
To create a schedule that fires every minute, run the following code:

from azure.ai.ml.entities import RecurrenceTrigger

schedule_name = "run_every_minute"

recurrence_trigger = RecurrenceTrigger(
    frequency="minute",
    interval=1,
)

To schedule a pipeline, you'll need pipeline_job to represent the pipeline you've built:

from azure.ai.ml.entities import JobSchedule

job_schedule = JobSchedule(
    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job
)

job_schedule = ml_client.schedules.begin_create_or_update(
    schedule=job_schedule
).result()

The display names of the jobs triggered by the schedule will be prefixed with the name of your schedule. 
You can review the jobs in the Azure Machine Learning studio:


To delete a schedule, you first need to disable it:

ml_client.schedules.begin_disable(name=schedule_name).result()
ml_client.schedules.begin_delete(name=schedule_name).result()